{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b64f2893ca4fffa",
   "metadata": {},
   "source": [
    "# MF backbone Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T09:39:56.677546Z",
     "start_time": "2024-09-22T09:39:52.488818Z"
    }
   },
   "outputs": [],
   "source": [
    "from model.MF import *\n",
    "from preprocess.AmazonBook import *\n",
    "from evaluation.MF_evaluation import *\n",
    "pd.options.display.max_rows = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = './dataset/amazon-book'\n",
    "dataset = AmazonBook(path)\n",
    "\n",
    "# Data(num_nodes=144242, edge_index=[2, 2380730], edge_label_index=[2, 603378])\n",
    "data = dataset.get()\n",
    "num_users, num_books = dataset.getNumber()\n",
    "config = {\n",
    "    'k': 20,\n",
    "    'learning_rate': 1e-5,  # over-fitting\n",
    "    'epochs': 120,\n",
    "    'num_layers': 2,\n",
    "    'batch_size': 8192,\n",
    "    'embedding_dim': 64,\n",
    "    'num_users': num_users,\n",
    "    'num_books': num_books,\n",
    "    'tuning_type': None,\n",
    "    \"weight_decay\": 1e-7,\n",
    "    'global_bias':(data.edge_index.size(1) + data.edge_label_index.size(1) + 2) / (num_books * num_users)\n",
    "}\n",
    "model = MF(\n",
    "    num_users= config['num_users'],\n",
    "    num_items= config['num_books'],\n",
    "    mean = config['global_bias'],\n",
    "    embedding_dim = config['embedding_dim']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a7f02e7d86772a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T09:40:20.440214Z",
     "start_time": "2024-09-22T09:40:09.233209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120, Train Loss: 0.9966, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 2/120, Train Loss: 0.9918, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 3/120, Train Loss: 0.9869, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 4/120, Train Loss: 0.9821, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 5/120, Train Loss: 0.9773, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 6/120, Train Loss: 0.9725, HR@20: 0.0001, Recall@20: 0.0002, NDCG@20: 0.0002\n",
      "Epoch 7/120, Train Loss: 0.9678, HR@20: 0.0001, Recall@20: 0.0003, NDCG@20: 0.0002\n",
      "Epoch 8/120, Train Loss: 0.9630, HR@20: 0.0002, Recall@20: 0.0003, NDCG@20: 0.0002\n",
      "Epoch 9/120, Train Loss: 0.9582, HR@20: 0.0002, Recall@20: 0.0003, NDCG@20: 0.0003\n",
      "Epoch 10/120, Train Loss: 0.9535, HR@20: 0.0003, Recall@20: 0.0004, NDCG@20: 0.0004\n",
      "Epoch 11/120, Train Loss: 0.9487, HR@20: 0.0004, Recall@20: 0.0006, NDCG@20: 0.0006\n",
      "Epoch 12/120, Train Loss: 0.9440, HR@20: 0.0007, Recall@20: 0.0010, NDCG@20: 0.0010\n",
      "Epoch 13/120, Train Loss: 0.9392, HR@20: 0.0011, Recall@20: 0.0018, NDCG@20: 0.0017\n",
      "Epoch 14/120, Train Loss: 0.9345, HR@20: 0.0015, Recall@20: 0.0026, NDCG@20: 0.0024\n",
      "Epoch 15/120, Train Loss: 0.9297, HR@20: 0.0020, Recall@20: 0.0034, NDCG@20: 0.0032\n",
      "Epoch 16/120, Train Loss: 0.9248, HR@20: 0.0025, Recall@20: 0.0044, NDCG@20: 0.0040\n",
      "Epoch 17/120, Train Loss: 0.9199, HR@20: 0.0029, Recall@20: 0.0052, NDCG@20: 0.0047\n",
      "Epoch 18/120, Train Loss: 0.9149, HR@20: 0.0033, Recall@20: 0.0060, NDCG@20: 0.0054\n",
      "Epoch 19/120, Train Loss: 0.9099, HR@20: 0.0037, Recall@20: 0.0068, NDCG@20: 0.0060\n",
      "Epoch 20/120, Train Loss: 0.9047, HR@20: 0.0041, Recall@20: 0.0076, NDCG@20: 0.0067\n",
      "Epoch 21/120, Train Loss: 0.8994, HR@20: 0.0044, Recall@20: 0.0081, NDCG@20: 0.0071\n",
      "Epoch 22/120, Train Loss: 0.8940, HR@20: 0.0046, Recall@20: 0.0087, NDCG@20: 0.0075\n",
      "Epoch 23/120, Train Loss: 0.8884, HR@20: 0.0048, Recall@20: 0.0093, NDCG@20: 0.0078\n",
      "Epoch 24/120, Train Loss: 0.8827, HR@20: 0.0049, Recall@20: 0.0097, NDCG@20: 0.0081\n",
      "Epoch 25/120, Train Loss: 0.8769, HR@20: 0.0051, Recall@20: 0.0100, NDCG@20: 0.0083\n",
      "Epoch 26/120, Train Loss: 0.8708, HR@20: 0.0051, Recall@20: 0.0103, NDCG@20: 0.0085\n",
      "Epoch 27/120, Train Loss: 0.8646, HR@20: 0.0052, Recall@20: 0.0105, NDCG@20: 0.0087\n",
      "Epoch 28/120, Train Loss: 0.8582, HR@20: 0.0052, Recall@20: 0.0106, NDCG@20: 0.0087\n",
      "Epoch 29/120, Train Loss: 0.8516, HR@20: 0.0053, Recall@20: 0.0107, NDCG@20: 0.0088\n",
      "Epoch 30/120, Train Loss: 0.8448, HR@20: 0.0053, Recall@20: 0.0108, NDCG@20: 0.0088\n",
      "Epoch 31/120, Train Loss: 0.8379, HR@20: 0.0053, Recall@20: 0.0108, NDCG@20: 0.0088\n",
      "Epoch 32/120, Train Loss: 0.8307, HR@20: 0.0053, Recall@20: 0.0109, NDCG@20: 0.0089\n",
      "Epoch 33/120, Train Loss: 0.8234, HR@20: 0.0053, Recall@20: 0.0109, NDCG@20: 0.0089\n",
      "Epoch 34/120, Train Loss: 0.8159, HR@20: 0.0053, Recall@20: 0.0109, NDCG@20: 0.0088\n",
      "Epoch 35/120, Train Loss: 0.8082, HR@20: 0.0053, Recall@20: 0.0109, NDCG@20: 0.0088\n",
      "Epoch 36/120, Train Loss: 0.8004, HR@20: 0.0053, Recall@20: 0.0109, NDCG@20: 0.0088\n",
      "Epoch 37/120, Train Loss: 0.7923, HR@20: 0.0053, Recall@20: 0.0108, NDCG@20: 0.0088\n",
      "Epoch 38/120, Train Loss: 0.7841, HR@20: 0.0052, Recall@20: 0.0108, NDCG@20: 0.0087\n",
      "Epoch 39/120, Train Loss: 0.7758, HR@20: 0.0052, Recall@20: 0.0108, NDCG@20: 0.0087\n",
      "Epoch 40/120, Train Loss: 0.7672, HR@20: 0.0052, Recall@20: 0.0107, NDCG@20: 0.0087\n",
      "Epoch 41/120, Train Loss: 0.7586, HR@20: 0.0052, Recall@20: 0.0107, NDCG@20: 0.0087\n",
      "Epoch 42/120, Train Loss: 0.7497, HR@20: 0.0052, Recall@20: 0.0107, NDCG@20: 0.0087\n",
      "Epoch 43/120, Train Loss: 0.7408, HR@20: 0.0052, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 44/120, Train Loss: 0.7317, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 45/120, Train Loss: 0.7224, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 46/120, Train Loss: 0.7131, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 47/120, Train Loss: 0.7036, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 48/120, Train Loss: 0.6941, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 49/120, Train Loss: 0.6844, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 50/120, Train Loss: 0.6746, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 51/120, Train Loss: 0.6647, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 52/120, Train Loss: 0.6548, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 53/120, Train Loss: 0.6448, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 54/120, Train Loss: 0.6347, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 55/120, Train Loss: 0.6245, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 56/120, Train Loss: 0.6144, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 57/120, Train Loss: 0.6041, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 58/120, Train Loss: 0.5938, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 59/120, Train Loss: 0.5836, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 60/120, Train Loss: 0.5732, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 61/120, Train Loss: 0.5629, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 62/120, Train Loss: 0.5526, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 63/120, Train Loss: 0.5423, HR@20: 0.0051, Recall@20: 0.0105, NDCG@20: 0.0085\n",
      "Epoch 64/120, Train Loss: 0.5320, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 65/120, Train Loss: 0.5217, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 66/120, Train Loss: 0.5115, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 67/120, Train Loss: 0.5013, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 68/120, Train Loss: 0.4911, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 69/120, Train Loss: 0.4810, HR@20: 0.0051, Recall@20: 0.0106, NDCG@20: 0.0086\n",
      "Epoch 70/120, Train Loss: 0.4710, HR@20: 0.0051, Recall@20: 0.0107, NDCG@20: 0.0086\n",
      "Epoch 71/120, Train Loss: 0.4611, HR@20: 0.0051, Recall@20: 0.0107, NDCG@20: 0.0087\n",
      "Epoch 72/120, Train Loss: 0.4513, HR@20: 0.0052, Recall@20: 0.0107, NDCG@20: 0.0087\n",
      "Epoch 73/120, Train Loss: 0.4415, HR@20: 0.0052, Recall@20: 0.0108, NDCG@20: 0.0087\n",
      "Epoch 74/120, Train Loss: 0.4319, HR@20: 0.0052, Recall@20: 0.0108, NDCG@20: 0.0088\n",
      "Epoch 75/120, Train Loss: 0.4223, HR@20: 0.0052, Recall@20: 0.0109, NDCG@20: 0.0089\n",
      "Epoch 76/120, Train Loss: 0.4129, HR@20: 0.0053, Recall@20: 0.0110, NDCG@20: 0.0089\n",
      "Epoch 77/120, Train Loss: 0.4036, HR@20: 0.0053, Recall@20: 0.0111, NDCG@20: 0.0090\n",
      "Epoch 78/120, Train Loss: 0.3944, HR@20: 0.0053, Recall@20: 0.0111, NDCG@20: 0.0091\n",
      "Epoch 79/120, Train Loss: 0.3853, HR@20: 0.0054, Recall@20: 0.0113, NDCG@20: 0.0092\n",
      "Epoch 80/120, Train Loss: 0.3764, HR@20: 0.0055, Recall@20: 0.0115, NDCG@20: 0.0094\n",
      "Epoch 81/120, Train Loss: 0.3677, HR@20: 0.0056, Recall@20: 0.0117, NDCG@20: 0.0095\n",
      "Epoch 82/120, Train Loss: 0.3590, HR@20: 0.0057, Recall@20: 0.0120, NDCG@20: 0.0097\n",
      "Epoch 83/120, Train Loss: 0.3505, HR@20: 0.0058, Recall@20: 0.0122, NDCG@20: 0.0099\n",
      "Epoch 84/120, Train Loss: 0.3422, HR@20: 0.0060, Recall@20: 0.0126, NDCG@20: 0.0102\n",
      "Epoch 85/120, Train Loss: 0.3340, HR@20: 0.0061, Recall@20: 0.0129, NDCG@20: 0.0105\n",
      "Epoch 86/120, Train Loss: 0.3259, HR@20: 0.0062, Recall@20: 0.0131, NDCG@20: 0.0108\n",
      "Epoch 87/120, Train Loss: 0.3181, HR@20: 0.0062, Recall@20: 0.0133, NDCG@20: 0.0110\n",
      "Epoch 88/120, Train Loss: 0.3103, HR@20: 0.0063, Recall@20: 0.0135, NDCG@20: 0.0112\n",
      "Epoch 89/120, Train Loss: 0.3027, HR@20: 0.0064, Recall@20: 0.0138, NDCG@20: 0.0114\n",
      "Epoch 90/120, Train Loss: 0.2953, HR@20: 0.0065, Recall@20: 0.0139, NDCG@20: 0.0116\n",
      "Epoch 91/120, Train Loss: 0.2880, HR@20: 0.0065, Recall@20: 0.0140, NDCG@20: 0.0117\n",
      "Epoch 92/120, Train Loss: 0.2808, HR@20: 0.0065, Recall@20: 0.0142, NDCG@20: 0.0118\n",
      "Epoch 93/120, Train Loss: 0.2738, HR@20: 0.0066, Recall@20: 0.0144, NDCG@20: 0.0118\n",
      "Epoch 94/120, Train Loss: 0.2670, HR@20: 0.0068, Recall@20: 0.0148, NDCG@20: 0.0120\n",
      "Epoch 95/120, Train Loss: 0.2603, HR@20: 0.0069, Recall@20: 0.0151, NDCG@20: 0.0122\n",
      "Epoch 96/120, Train Loss: 0.2537, HR@20: 0.0070, Recall@20: 0.0153, NDCG@20: 0.0122\n",
      "Epoch 97/120, Train Loss: 0.2473, HR@20: 0.0070, Recall@20: 0.0153, NDCG@20: 0.0123\n",
      "Epoch 98/120, Train Loss: 0.2410, HR@20: 0.0070, Recall@20: 0.0153, NDCG@20: 0.0123\n",
      "Epoch 99/120, Train Loss: 0.2349, HR@20: 0.0070, Recall@20: 0.0153, NDCG@20: 0.0123\n",
      "Epoch 100/120, Train Loss: 0.2289, HR@20: 0.0070, Recall@20: 0.0154, NDCG@20: 0.0124\n",
      "Epoch 101/120, Train Loss: 0.2230, HR@20: 0.0070, Recall@20: 0.0155, NDCG@20: 0.0123\n",
      "Epoch 102/120, Train Loss: 0.2173, HR@20: 0.0070, Recall@20: 0.0154, NDCG@20: 0.0123\n",
      "Epoch 103/120, Train Loss: 0.2117, HR@20: 0.0070, Recall@20: 0.0154, NDCG@20: 0.0123\n",
      "Epoch 104/120, Train Loss: 0.2062, HR@20: 0.0070, Recall@20: 0.0155, NDCG@20: 0.0123\n",
      "Epoch 105/120, Train Loss: 0.2009, HR@20: 0.0069, Recall@20: 0.0154, NDCG@20: 0.0122\n",
      "Epoch 106/120, Train Loss: 0.1957, HR@20: 0.0069, Recall@20: 0.0154, NDCG@20: 0.0122\n",
      "Epoch 107/120, Train Loss: 0.1906, HR@20: 0.0068, Recall@20: 0.0152, NDCG@20: 0.0121\n",
      "Epoch 108/120, Train Loss: 0.1856, HR@20: 0.0067, Recall@20: 0.0151, NDCG@20: 0.0121\n",
      "Epoch 109/120, Train Loss: 0.1807, HR@20: 0.0067, Recall@20: 0.0150, NDCG@20: 0.0120\n",
      "Epoch 110/120, Train Loss: 0.1760, HR@20: 0.0065, Recall@20: 0.0147, NDCG@20: 0.0118\n",
      "Epoch 111/120, Train Loss: 0.1713, HR@20: 0.0064, Recall@20: 0.0144, NDCG@20: 0.0117\n",
      "Epoch 112/120, Train Loss: 0.1668, HR@20: 0.0063, Recall@20: 0.0143, NDCG@20: 0.0116\n",
      "Epoch 113/120, Train Loss: 0.1624, HR@20: 0.0062, Recall@20: 0.0141, NDCG@20: 0.0116\n",
      "Epoch 114/120, Train Loss: 0.1581, HR@20: 0.0061, Recall@20: 0.0139, NDCG@20: 0.0115\n",
      "Epoch 115/120, Train Loss: 0.1539, HR@20: 0.0061, Recall@20: 0.0138, NDCG@20: 0.0114\n",
      "Epoch 116/120, Train Loss: 0.1498, HR@20: 0.0060, Recall@20: 0.0137, NDCG@20: 0.0114\n",
      "Epoch 117/120, Train Loss: 0.1458, HR@20: 0.0060, Recall@20: 0.0137, NDCG@20: 0.0114\n",
      "Epoch 118/120, Train Loss: 0.1419, HR@20: 0.0060, Recall@20: 0.0136, NDCG@20: 0.0114\n",
      "Epoch 119/120, Train Loss: 0.1380, HR@20: 0.0060, Recall@20: 0.0137, NDCG@20: 0.0114\n",
      "Epoch 120/120, Train Loss: 0.1343, HR@20: 0.0060, Recall@20: 0.0137, NDCG@20: 0.0114\n",
      "Total time: 4437.63s\n"
     ]
    }
   ],
   "source": [
    "model, recall, ndcg = MF_based_eva(model, config, data, device)\n",
    "# Store the model parameters\n",
    "torch.save(model.state_dict(), f\"MF_Amazon_Book_{config['epochs']}_Epochs_Top_{config['k']}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
